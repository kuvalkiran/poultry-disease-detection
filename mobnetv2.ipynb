{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYPZqmN_UiU0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mydrive')\n",
        "\n",
        "sdir=r''   # Update with your directory path to the train folder\n",
        "csvpath=r''   # Update with your file path to the csv folder\n",
        "\n",
        "# Read CSV into DataFrame\n",
        "df = pd.read_csv(csvpath)\n",
        "df.columns = ['filepaths', 'labels']\n",
        "df['filepaths'] = df['filepaths'].apply(lambda x: os.path.join(sdir, x))\n",
        "\n",
        "# Split the dataset into train, test, and validation sets\n",
        "train_df, dummy_df = train_test_split(df, train_size=0.9, shuffle=True, random_state=123, stratify=df['labels'])\n",
        "test_df, valid_df = train_test_split(dummy_df, test_size=0.5, shuffle=True, random_state=123, stratify=dummy_df['labels'])\n",
        "\n",
        "# Define a function to trim dataset based on sample size\n",
        "def trim(df, max_size, min_size, column):\n",
        "    # Group dataframe by column\n",
        "    groups = df.groupby(column)\n",
        "    sample_list = []\n",
        "    for label in df[column].unique():\n",
        "        group = groups.get_group(label)\n",
        "        sample_count = len(group)\n",
        "        if sample_count > max_size:\n",
        "            strat = group[column]\n",
        "            samples, _ = train_test_split(group, train_size=max_size, shuffle=True, random_state=123, stratify=strat)\n",
        "            sample_list.append(samples)\n",
        "        elif min_size <= sample_count <= max_size:\n",
        "            sample_list.append(group)\n",
        "    return pd.concat(sample_list, axis=0).reset_index(drop=True)\n",
        "\n",
        "# Trim the training dataset\n",
        "max_samples = 500\n",
        "min_samples = 0\n",
        "train_df = trim(train_df, max_samples, min_samples, 'labels')\n",
        "\n",
        "# Set image size and other parameters\n",
        "img_size = (224, 224)\n",
        "batch_size = 20\n",
        "\n",
        "# Data generators\n",
        "train_gen = ImageDataGenerator(horizontal_flip=True, rotation_range=20, width_shift_range=0.2,\n",
        "                               height_shift_range=0.2, zoom_range=0.2)\n",
        "valid_gen = ImageDataGenerator()\n",
        "\n",
        "train_data = train_gen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
        "                                           class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
        "valid_data = valid_gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
        "                                           class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size)\n",
        "\n",
        "# Define the base model\n",
        "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(img_size[0], img_size[1], 3), pooling='max')\n",
        "base_model.trainable = True\n",
        "\n",
        "# Add custom layers on top of base model\n",
        "x = base_model.output\n",
        "x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\n",
        "x = Dense(1024, kernel_regularizer=regularizers.l2(0.016), activity_regularizer=regularizers.l1(0.006),\n",
        "          bias_regularizer=regularizers.l1(0.006), activation='relu')(x)\n",
        "x = Dropout(rate=0.3, seed=123)(x)\n",
        "x = Dense(1024, kernel_regularizer=regularizers.l2(0.016), activity_regularizer=regularizers.l1(0.006),\n",
        "          bias_regularizer=regularizers.l1(0.006), activation='relu')(x)\n",
        "x = Dropout(rate=0.45, seed=123)(x)\n",
        "output = Dense(len(train_data.class_indices), activation='softmax')(x)\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "lr = 0.001\n",
        "model.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, verbose=1, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_data, epochs=25, verbose=1, callbacks=callbacks, validation_data=valid_data)\n",
        "\n",
        "# Plot training history\n",
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy / Loss')\n",
        "    plt.title('Training and Validation Metrics')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_data = valid_gen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
        "                                          class_mode='categorical', color_mode='rgb', shuffle=False,\n",
        "                                          batch_size=len(test_df))  # Adjust batch_size\n",
        "\n",
        "y_pred = model.predict(test_data)\n",
        "y_true = test_data.labels\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Generate classification report\n",
        "classes = list(train_data.class_indices.keys())\n",
        "clr = classification_report(y_true, y_pred_labels, target_names=classes)\n",
        "print(\"Classification Report:\\n\", clr)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subject='temp'\n",
        "acc=str(( 0.87) * 100)\n",
        "index=acc.rfind('.')\n",
        "acc=acc[:index + 3]\n",
        "save_id= subject + '_' + str(acc) + '.h5'\n",
        "save_modelpath = # Update with path to save model\n",
        "model_save_loc=os.path.join('save_modelpath', save_id)\n",
        "model.save(model_save_loc)\n",
        "print ('model was saved as ' , model_save_loc )"
      ],
      "metadata": {
        "id": "lgOn8LC_Uu3C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}